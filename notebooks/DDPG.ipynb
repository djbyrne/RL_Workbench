{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import time\n",
    "import gym\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import collections\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float32_preprocessor(states):\n",
    "    \"\"\"\n",
    "    Convert list of states into the form suitable for model. By default we assume Variable\n",
    "\n",
    "    Args:\n",
    "        states: list of numpy arrays with states\n",
    "\n",
    "    Returns:\n",
    "        cleaned variable in the form of np.float32\n",
    "    \"\"\"\n",
    "\n",
    "    np_states = np.array(states, dtype=np.float32)\n",
    "    return torch.tensor(np_states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGActor(nn.Module):\n",
    "    def __init__(self, obs_size, act_size):\n",
    "        super(DDPGActor, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, act_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGCritic(nn.Module):\n",
    "    def __init__(self, obs_size, act_size):\n",
    "        super(DDPGCritic, self).__init__()\n",
    "\n",
    "        self.obs_net = nn.Sequential(\n",
    "            nn.Linear(obs_size, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.out_net = nn.Sequential(\n",
    "            nn.Linear(128 + act_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        obs = self.obs_net(x)\n",
    "        return self.out_net(torch.cat([obs, a], dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetNetwork:\n",
    "    \"\"\"\n",
    "    Wrapper around model which provides copy of it instead of trained weights\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.target_model = copy.deepcopy(model)\n",
    "\n",
    "    def sync(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def alpha_sync(self, alpha):\n",
    "        \"\"\"\n",
    "        Blend params of target net with params from the model\n",
    "        :param alpha:\n",
    "        \"\"\"\n",
    "        assert isinstance(alpha, float)\n",
    "        assert 0.0 < alpha <= 1.0\n",
    "        \n",
    "        state = self.model.state_dict()\n",
    "        tgt_state = self.target_model.state_dict()\n",
    "        for k, v in state.items():\n",
    "            tgt_state[k] = tgt_state[k] * alpha + (1 - alpha) * v\n",
    "\n",
    "        self.target_model.load_state_dict(tgt_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, state_dims, action_dims, buffer=1000000, min_buffer=50000, batch=64):\n",
    "        self.buffer_size = buffer\n",
    "        self.min_buffer_size = min_buffer\n",
    "        self.state_dims = state_dims\n",
    "        self.action_dims = action_dims\n",
    "        self.batch_size = batch\n",
    "        self.count = 0\n",
    "        self.current = 0\n",
    "\n",
    "        # preallocate memory\n",
    "        self.actions = np.empty((self.buffer_size,) + self.action_dims, dtype = np.float32)\n",
    "        self.rewards = np.empty(self.buffer_size, dtype = np.float32)\n",
    "        self.states = np.empty((self.buffer_size,) + self.state_dims, dtype = np.float32)\n",
    "        self.terminals = np.empty(self.buffer_size, dtype = np.bool)   \n",
    "        \n",
    "        self.state_batch = np.empty((self.batch_size,) + self.state_dims, dtype = np.float32)\n",
    "        self.next_state_batch = np.empty((self.batch_size,) + self.state_dims, dtype = np.float32)\n",
    "        \n",
    "        \n",
    "    def add(self, action, reward, state, terminal):        \n",
    "        assert state.shape == self.state_dims\n",
    "        assert action.shape == self.action_dims\n",
    "\n",
    "        self.actions[self.current, ...] = action\n",
    "        self.rewards[self.current] = reward\n",
    "        self.states[self.current, ...] = state\n",
    "        self.terminals[self.current] = terminal\n",
    "        self.count = max(self.count, self.current + 1)\n",
    "        self.current = (self.current + 1) % self.buffer_size\n",
    "        \n",
    "  \n",
    "    def getState(self, index):\n",
    "        # Returns the state at position 'index'.\n",
    "        return self.states[index, ...]\n",
    "         \n",
    "\n",
    "    def getMinibatch(self):\n",
    "        # memory should be initially populated with random actions up to 'min_buffer_size'\n",
    "        assert self.count >= self.min_buffer_size, \"Replay memory does not contain enough samples to start learning, take random actions to populate replay memory\"\n",
    "                \n",
    "        # sample random indexes\n",
    "        indexes = []\n",
    "        # do until we have a full batch of states\n",
    "        while len(indexes) < self.batch_size:\n",
    "            # find random index \n",
    "            while True:\n",
    "                # sample one index\n",
    "                index = np.random.randint(1, self.count)\n",
    "                # check index is ok\n",
    "                # if state and next state wrap over current pointer, then get new one (as state from current pointer position will not be from same episode as state from previous position)\n",
    "                if index == self.current:\n",
    "                    continue\n",
    "                # if state and next state wrap over episode end, i.e. current state is terminal, then get new one (note that next state can be terminal)\n",
    "                if self.terminals[index-1]:\n",
    "                    continue\n",
    "                # index is ok to use\n",
    "                break\n",
    "            \n",
    "            # Populate states and next_states with selected state and next_state\n",
    "            # NB! having index first is fastest in C-order matrices\n",
    "            self.state_batch[len(indexes), ...] = self.getState(index - 1)\n",
    "            self.next_state_batch[len(indexes), ...] = self.getState(index)\n",
    "            indexes.append(index)   \n",
    "        \n",
    "        actions = self.actions[indexes]\n",
    "        rewards = self.rewards[indexes]\n",
    "        terminals = self.terminals[indexes]\n",
    "        \n",
    "        return self.state_batch, actions, rewards, self.next_state_batch, terminals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrnsteinUhlenbeckActionNoise:\n",
    "    def __init__(self, mu, sigma=0.3, theta=0.15, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observe(env, replay_mem):\n",
    "    sys.stdout.write('\\nPopulating replay memory with random actions...\\n')   \n",
    "    sys.stdout.flush()          \n",
    "    env.reset()\n",
    "\n",
    "    for random_step in range(1, OBSERVATION+1):\n",
    "        if RENDER:\n",
    "            env.render()\n",
    "\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, terminal, _ = env.step(action)\n",
    "        replay_mem.add(action, reward, state, terminal)\n",
    "\n",
    "        if terminal:\n",
    "            env.reset()\n",
    "\n",
    "        if random_step % 1000 == 0:\n",
    "            sys.stdout.write('\\x1b[2K\\rStep {:d}/{:d}'.format(random_step, OBSERVATION))\n",
    "            sys.stdout.flush() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_networks(device, act_net, crt_net, act_opt, crt_opt, replay_mem, gamma=0.99):\n",
    "\n",
    "    # Get minibatch\n",
    "    states_batch, actions_batch, rewards_batch, next_states_batch, terminals_batch = replay_mem.getMinibatch()\n",
    "    \n",
    "    \n",
    "    print()\n",
    "    \n",
    "    #preprocess \n",
    "    \n",
    "    states, actions, rewards, dones, last_states = [], [], [], [], []\n",
    "    \n",
    "    for s,a,r,s_,d in zip(states_batch, actions_batch, rewards_batch, next_states_batch, terminals_batch):\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        dones.append(s_ is None)\n",
    "        if s_ is None:\n",
    "            last_states.append(s)\n",
    "        else:\n",
    "            last_states.append(s_)\n",
    "            \n",
    "            \n",
    "    states_v = float32_preprocessor(states).to(device)\n",
    "    actions_v = float32_preprocessor(actions).to(device)\n",
    "    rewards_v = float32_preprocessor(rewards).to(device)\n",
    "    last_states_v = float32_preprocessor(last_states).to(device)\n",
    "    dones_t = float32_preprocessor(dones).to(device)\n",
    "  \n",
    "    \n",
    "    # Critic training step \n",
    "    actions_next = tgt_act_net.target_model(last_states_v)\n",
    "    Q_targets_next = tgt_crt_net.target_model(last_states_v, actions_next)\n",
    "    \n",
    "    Q_targets = rewards_v + (gamma * Q_targets_next * (1 - dones_t))\n",
    "    \n",
    "    Q_expected = crt_net(states_v, actions_v)\n",
    "    \n",
    "    critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "    \n",
    "    crt_opt.zero_grad()\n",
    "    critic_loss.backward()\n",
    "#     torch.nn.utils.clip_grad_norm_(crt_net.parameters(), 1)\n",
    "    crt_opt.step()\n",
    "    \n",
    "#     crt_opt.zero_grad()\n",
    "#     # Predict actions for next states by passing next states through policy target network\n",
    "#     future_action = tgt_act_net.target_model(last_states_v)\n",
    "#     q_value = crt_net(states_v, actions_v)\n",
    "#     # Predict target Q values by passing next states and actions through value target network\n",
    "# #     future_Q = sess.run(critic_target.output, {state_ph:next_states_batch, action_ph:future_action})[:,0]\n",
    "#     future_Q = tgt_crt_net.target_model(last_states_v, future_action)\n",
    "#     # Q values of the terminal states is 0 by definition\n",
    "#     future_Q[dones_t] = 0\n",
    "#     targets = rewards_v + (future_Q * gamma)\n",
    "    \n",
    "#     critic_loss = F.mse_loss(q_value, future_Q)\n",
    "#     critic_loss.backward()\n",
    "#     crt_opt.step()\n",
    "    \n",
    "    \n",
    "    # Actor training step\n",
    "    actions_pred = act_net(states_v)\n",
    "    actor_loss = -crt_net(states_v, actions_pred).mean()\n",
    "    act_opt.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    act_opt.step()\n",
    "    \n",
    "#     act_opt.zero_grad()\n",
    "#     # Get policy network's action outputs for selected states\n",
    "#     actor_actions = act_net(states_v)\n",
    "#     actions_pred = act_net(states_v)\n",
    "#     actor_loss = -crt_net(states_v, actions_pred).mean()\n",
    "#     # Minimize the loss\n",
    "#     actor_loss.backward()\n",
    "#     act_opt.step()\n",
    "\n",
    "    tgt_act_net.alpha_sync(alpha=TAU)\n",
    "    tgt_crt_net.alpha_sync(alpha=TAU)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"msinto\"\n",
    "ENV = \"Pendulum-v0\"\n",
    "SEED = 99999999\n",
    "NOISE_SCALE = 0.1\n",
    "CUDA = False\n",
    "LRA = 0.0001\n",
    "LRC = 0.001\n",
    "RENDER = False\n",
    "OBSERVATION = 50000\n",
    "EXPLORATION = 1000\n",
    "SAVE_CP = 200\n",
    "TAU = 0.001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDPGActor(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "    (5): Tanh()\n",
      "  )\n",
      ")\n",
      "DDPGCritic(\n",
      "  (obs_net): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (out_net): Sequential(\n",
      "    (0): Linear(in_features=129, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create Environment\n",
    "env = gym.make(ENV)\n",
    "state_dims = env.observation_space.shape\n",
    "action_dims = env.action_space.shape\n",
    "action_bound_low = env.action_space.low\n",
    "action_bound_high = env.action_space.high\n",
    "\n",
    "# Set random seeds for reproducability\n",
    "env.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Initialise replay memory\n",
    "replay_mem = ReplayMemory(state_dims, action_dims)\n",
    "\n",
    "# Initialise Ornstein-Uhlenbeck Noise generator\n",
    "exploration_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(action_dims))\n",
    "noise_scaling = NOISE_SCALE * (action_bound_high - action_bound_low)\n",
    "\n",
    "# Networks\n",
    "device = torch.device(\"cuda\" if CUDA else \"cpu\")\n",
    "\n",
    "act_net = DDPGActor(env.observation_space.shape[0], env.action_space.shape[0]).to(device)\n",
    "crt_net = DDPGCritic(env.observation_space.shape[0], env.action_space.shape[0]).to(device)\n",
    "\n",
    "\n",
    "print(act_net)\n",
    "print(crt_net)\n",
    "\n",
    "tgt_act_net = TargetNetwork(act_net)\n",
    "tgt_crt_net = TargetNetwork(crt_net)\n",
    "\n",
    "# setup save directory\n",
    "save_path = os.path.join(\"saves\", \"ddpg-\" + NAME)\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# network optimizers\n",
    "act_opt = optim.Adam(act_net.parameters(), lr=LRA)\n",
    "crt_opt = optim.Adam(crt_net.parameters(), lr=LRC)\n",
    "\n",
    "# Writer\n",
    "writer = SummaryWriter(comment=\"-ddpg_\" + NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Populating replay memory with random actions...\n",
      "Step 50000/50000[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\n",
      "\n",
      "Training...\n",
      "Episode 60/1000 \t Avg Reward = -1522.868 \t Reward = -1196.262 \t (0.005 s/step)[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9dfa33dda5ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mupdate_networks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrt_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrt_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_mem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-2a9f0b489835>\u001b[0m in \u001b[0;36mupdate_networks\u001b[0;34m(device, act_net, crt_net, act_opt, crt_opt, replay_mem, gamma)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mcrt_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mcritic_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;31m#     torch.nn.utils.clip_grad_norm_(crt_net.parameters(), 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mcrt_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl_dev/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl_dev/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initially populate replay memory by taking random actions \n",
    "observe(env, replay_mem)\n",
    "\n",
    "sys.stdout.write('\\n\\nTraining...\\n')   \n",
    "sys.stdout.flush()\n",
    "\n",
    "start_ep = 0\n",
    "total_rewards = []\n",
    "\n",
    "for train_ep in range(start_ep+1, EXPLORATION+1):      \n",
    "    # Reset environment and noise process\n",
    "    state = env.reset()\n",
    "    exploration_noise.reset()\n",
    "    \n",
    "    train_step = 0\n",
    "    episode_reward = 0\n",
    "    duration_values = []\n",
    "    ep_done = False\n",
    "    \n",
    "    \n",
    "#     sys.stdout.write('\\n')   \n",
    "#     sys.stdout.flush()\n",
    "\n",
    "    while not ep_done:\n",
    "        train_step += 1\n",
    "        start_time = time.time()            \n",
    "       \n",
    "        action = act_net(float32_preprocessor(state).to(device)).cpu().data.numpy()\n",
    "        action += exploration_noise() * noise_scaling\n",
    "        \n",
    "        state, reward, terminal, _ = env.step(action)\n",
    "        replay_mem.add(action, reward, state, terminal)\n",
    "        \n",
    "        episode_reward += reward \n",
    "            \n",
    "        update_networks(device, act_net, crt_net, act_opt, crt_opt, replay_mem)\n",
    "        \n",
    "        \n",
    "        # Display progress            \n",
    "        duration = time.time() - start_time\n",
    "        duration_values.append(duration)\n",
    "        ave_duration = sum(duration_values)/float(len(duration_values))\n",
    "        \n",
    "        \n",
    "            \n",
    "        if terminal:\n",
    "            total_rewards.append(episode_reward)\n",
    "            reward100 = total_rewards[-100:]\n",
    "            avg_reward = sum(reward100)/len(reward100)\n",
    "            sys.stdout.write('\\x1b[2K\\rEpisode {:d}/{:d} \\t Avg Reward = {:.3f} \\t Reward = {:.3f} \\t ({:.3f} s/step)'.format(train_ep, EXPLORATION, avg_reward, episode_reward, ave_duration))\n",
    "            sys.stdout.flush() \n",
    "            ep_done = True\n",
    "            \n",
    "    \n",
    "    if train_ep % SAVE_CP == 0:\n",
    "        name = \"ep_%d.dat\" % (train_ep)\n",
    "        fname = os.path.join(save_path, name)\n",
    "        torch.save(act_net.state_dict(), fname)\n",
    "        sys.stdout.write('\\n Checkpoint saved.')   \n",
    "        sys.stdout.flush() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
